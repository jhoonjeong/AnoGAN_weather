{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time detection of anomaly signal of weather data using GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import os\n",
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data for preliminary analysis contains two years observation weather data from 38 currently working stations in Pennsylvania and consists over million time-series with 90 columns of hourly, daily and monthly observed values. For simplicity, I mangled data containing only dry bulb temperature value with 38 station columns and over 18000 hourly lines of 2018 and 2017 to train GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/media/jeong/Linux_DATA/data/\"\n",
    "\n",
    "def remove_str(x):\n",
    "    str_filter = re.compile(\"^[-+0-9.]\")\n",
    "    x = ''.join(list(filter(str_filter.match, str(x))))\n",
    "    if len(x)==0:\n",
    "        x = np.NAN\n",
    "    return x\n",
    "\n",
    "r = re.compile('HOURLY.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_data_process(f_idx, year):\n",
    "    df_test = []\n",
    "    for f_id in f_idx:\n",
    "        df_test.append(pd.read_csv(datadir + 'incubator/NOAA/' + str(f_id) + '.csv', low_memory=False))\n",
    "    df_final = pd.concat(df_test)\n",
    "\n",
    "    df_drytemp = df_final[['DATE', 'STATION', 'HOURLYDRYBULBTEMPC']]\n",
    "    df_drytemp.loc[:, 'HOURLYDRYBULBTEMPC'] = df_drytemp.HOURLYDRYBULBTEMPC.apply(remove_str)\n",
    "    df_drytemp = df_drytemp[df_final.HOURLYDRYBULBTEMPC.notna()]\n",
    "    df_drytemp.loc[:, 'HOURLYDRYBULBTEMPC'] = df_drytemp.HOURLYDRYBULBTEMPC.astype(float).values\n",
    "\n",
    "    r = re.compile('HOURLY.*')\n",
    "    sel_col_name = list(filter(r.match, df_drytemp.columns))\n",
    "\n",
    "    df_drytemp['NEW_DATE'] = pd.to_datetime(df_drytemp.DATE).apply(lambda x: (x - pd.to_datetime(str(year) + '-01-01 00:00'))//3600000000000).astype(int)\n",
    "\n",
    "    df_pivot = df_drytemp.pivot_table(index=['STATION'], columns=['NEW_DATE'], values='HOURLYDRYBULBTEMPC')\n",
    "\n",
    "    for i in df_pivot.columns.values:\n",
    "        df_pivot[i] = df_pivot[i].fillna(df_pivot[i].mean())\n",
    "\n",
    "    df_pivot = df_pivot.T\n",
    "    df_pivot.to_csv(datadir + 'incubator/NOAA/' + 'PEN' + str(year) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_2017_idx = [1597037, 1597038, 1597039, 1597040]\n",
    "f_2018_idx = [1596652, 1596691, 1596696, 1596958, 1597046]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeong/.env/tensorflow/lib/python3.5/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "hourly_data_process(f_2018_idx, 2018)\n",
    "hourly_data_process(f_2017_idx, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN consists with two parts, discriminator and generator. In this forecast GAN, the generator is trained to predict the dry temperature data of 't+3' slice only from 't'~'t+2' slice input. The discriminator gets the temperature data of 't+3' slice as true observed data or predicted synthetic data along with 't'~'t+2' data. The discriminator judges if the last slice of data is from generator or real value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # Allocated memory of GPU grow\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Net\n",
    "X1 = tf.placeholder(tf.float32, shape=[None, 114], name='X1')\n",
    "X2 = tf.placeholder(tf.float32, shape=[None, 38], name='X2')\n",
    "\n",
    "D_W1 = tf.get_variable('D_W1', shape=[152, 512], \n",
    "                       initializer=tf.glorot_uniform_initializer())\n",
    "D_b1 = tf.get_variable('D_b1', shape=[512], \n",
    "                       initializer=tf.glorot_uniform_initializer())\n",
    "#D_W2 = tf.get_variable('D_W2', shape=[512, 512], \n",
    "#                       initializer=tf.glorot_uniform_initializer())\n",
    "#D_b2 = tf.get_variable('D_b2', shape=[512], \n",
    "#                       initializer=tf.glorot_uniform_initializer())\n",
    "D_W3 = tf.get_variable('D_W2', shape=[512, 1], \n",
    "                       initializer=tf.glorot_uniform_initializer())\n",
    "D_b3 = tf.get_variable('D_b2', shape=[1], \n",
    "                       initializer=tf.glorot_uniform_initializer())\n",
    "\n",
    "#theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "theta_D = [D_W1, D_W3, D_b1, D_b3]\n",
    "\n",
    "# Generator Net\n",
    "\n",
    "G_W1 = tf.get_variable('G_W1', shape=[114, 128], \n",
    "                       initializer=tf.glorot_uniform_initializer())\n",
    "G_b1 = tf.get_variable('G_b1', shape=[128], \n",
    "                       initializer=tf.glorot_uniform_initializer())\n",
    "G_W2 = tf.get_variable('G_W2', shape=[128, 128], \n",
    "                       initializer=tf.glorot_uniform_initializer())\n",
    "G_b2 = tf.get_variable('G_b2', shape=[128], \n",
    "                       initializer=tf.glorot_uniform_initializer())\n",
    "G_W3 = tf.get_variable('G_W3', shape=[128, 38], \n",
    "                       initializer=tf.glorot_uniform_initializer())\n",
    "G_b3 = tf.get_variable('G_b3', shape=[38], \n",
    "                       initializer=tf.glorot_uniform_initializer())\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "\n",
    "\n",
    "def discriminator(x1, x2):\n",
    "    x = tf.concat([x1, x2], 1)\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "#    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "#    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "    D_logit = tf.matmul(D_h1, D_W3) + D_b3\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit\n",
    "\n",
    "\n",
    "def generator(x1):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(x1, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)\n",
    "    G_log_prob = tf.matmul(G_h2, G_W3) + G_b3\n",
    "    #G_prob = tf.nn.relu(G_log_prob)\n",
    "    G_prob = G_log_prob\n",
    "\n",
    "    return G_prob\n",
    "\n",
    "\n",
    "def fD(x1, x2, g_z):\n",
    "    D_real, D_logit_real = discriminator(x1, x2)\n",
    "    D_fake, D_logit_fake = discriminator(x1, g_z)\n",
    "    D_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real))\n",
    "    D_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake))\n",
    "    return D_loss_real + D_loss_fake\n",
    "\n",
    "\n",
    "G_sample = generator(X1)\n",
    "\n",
    "#D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "#G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "# Alternative losses:\n",
    "# -------------------\n",
    "\n",
    "D_fake0, D_logit_fake0 = discriminator(X1, G_sample)\n",
    "\n",
    "D_loss = tf.reduce_mean(fD(X1, X2, G_sample))\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake0, labels=tf.ones_like(D_logit_fake0)))\n",
    "\n",
    "# Only update D(X)'s parameters, so var_list = theta_D\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "# Only update G(X)'s parameters, so var_list = theta_G\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "mb_size=64 # minibatch\n",
    "\n",
    "D_loss_stack = []\n",
    "G_loss_stack = []\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use temperature data of 2018 to train forecast GAN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pen2018 = pd.read_csv(datadir + 'incubator/NOAA/' + 'PEN2018.csv', index_col='NEW_DATE')\n",
    "\n",
    "X_train = []\n",
    "X_real = []\n",
    "# Random selection of time slices\n",
    "idx = np.arange(len(df_pen2018)-3)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "for k in idx:\n",
    "    X_train.append(df_pen2018.iloc[k:k+3].values.flatten())\n",
    "    X_real.append(df_pen2018.iloc[k+3].values.flatten())\n",
    "\n",
    "n_learn = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in tnrange(n_learn, desc='GAN'):\n",
    "    idxs = np.random.choice(len(df_pen2018)-3, mb_size)\n",
    "    X_train_0 = [X_train[x] for x in idxs]\n",
    "    X_real_0 = [X_real[x] for x in idxs]\n",
    "\n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X1: X_train_0, X2: X_real_0})\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={X1: X_train_0})\n",
    "    D_loss_stack.append(D_loss_curr)\n",
    "    G_loss_stack.append(G_loss_curr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pic1. The loss functions are adversarial. Since we use limited number of training set, both of loss functions are decreasing. When we train this model with fluent data set, the picture will show adversarial relation between values of loss functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stack = len(D_loss_stack)\n",
    "plt_pts = 500\n",
    "t = range(plt_pts)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('x' + str(n_stack//plt_pts) + ' Learning')\n",
    "#ax1.set_xlabel('time (s)')\n",
    "ax1.set_ylabel('discriminator', color=color)\n",
    "ax1.plot(t, np.mean(np.array(D_loss_stack).reshape(plt_pts,n_stack//plt_pts), axis=1), color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('generator', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(t, np.mean(np.array(G_loss_stack).reshape(plt_pts,n_stack//plt_pts), axis=1), color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with the data of 2017\n",
    "### Predict the last slice of sequential time data using GAN network which is trained with the 2018 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = pd.read_csv(datadir + 'incubator/NOAA/' + 'PEN2017.csv', index_col='NEW_DATE')\n",
    "\n",
    "X_test = []\n",
    "X_ground = []\n",
    "idx = np.arange(len(df_2017)-3)\n",
    "np.random.shuffle(idx)\n",
    "for k in idx:\n",
    "    X_test.append(df_2017.iloc[k:k+3].values.flatten())\n",
    "    X_ground.append(df_2017.iloc[k+3].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test how much similar the predicted data with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sne_idx = np.random.choice(len(df_2017)-3, 200)\n",
    "input_set = [X_test[i] for i in t_sne_idx]\n",
    "real_set = [X_ground[i] for i in t_sne_idx]\n",
    "predicted_set = sess.run(generator(X1), feed_dict={X1 : input_set})\n",
    "\n",
    "df_real = pd.DataFrame(real_set)\n",
    "df_predicted = pd.DataFrame(predicted_set)\n",
    "df_real.columns = df_2017.columns\n",
    "df_predicted.columns = df_2017.columns\n",
    "df_real['real'] = 1\n",
    "df_predicted['real'] = 0\n",
    "\n",
    "df_tsne = df_real.append(df_predicted)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "intermediates_tsne = tsne.fit_transform(df_tsne.drop(['real'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pic2. The picture shows us that predicted(green) points mimic points of real data set. While, in the left upper region, predicted points are far from real data point,  predicted points are much similar with real values at the right bottom region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_intermediates = df_tsne['real'].values\n",
    "real_tsne = intermediates_tsne[color_intermediates==1]\n",
    "predicted_tsne = intermediates_tsne[color_intermediates==0]\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x=real_tsne[:,0], \n",
    "            y=real_tsne[:,1], s=10, \n",
    "            c='blue', \n",
    "            alpha=1., label='real')\n",
    "plt.scatter(x=predicted_tsne[:,0], \n",
    "            y=predicted_tsne[:,1], s=10, \n",
    "            c='lime', \n",
    "            alpha=1, label='predicted')\n",
    "plt.legend(loc='upper right', markerscale=2, fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "#df_allfeats['tsne0'] = intermediates_tsne[:,0]\n",
    "#df_allfeats['tsne1'] = intermediates_tsne[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following pictures are temperature maps of given time slice. Upper 4-pictures are drawn with real temperature data of each time slice and lower 1 picture is temperature predicted map with forecast GAN based on upper 't'~'t+2' input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "coord = pd.read_csv(datadir + 'incubator/NOAA/' + 'PEN_station.csv', index_col='STATION')\n",
    "grid_x, grid_y = np.mgrid[-80.5:-75.0:100j, 39.6:42.1:100j]\n",
    "x_coord = grid_x[:,0]\n",
    "y_coord = grid_x[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.random.randint(200)\n",
    "pics = np.hstack([input_set[rand_idx], real_set[rand_idx], predicted_set[rand_idx]]).reshape(5,len(coord))\n",
    "\n",
    "grid_z = []\n",
    "for pic in pics:\n",
    "    grid_z.append(griddata(coord.values, pic, (grid_x, grid_y), method='cubic'))\n",
    "\n",
    "v_min = pics.min()\n",
    "v_max = pics.max()\n",
    "\n",
    "label = ['t', 't+1', 't+2', 't+3 real', 'predicted']\n",
    "for i in range(5):\n",
    "    if i==4:\n",
    "        j=8\n",
    "    else:\n",
    "        j=i+1\n",
    "    plt.subplot(2,4,j)\n",
    "    plt.imshow(grid_z[i].T, origin='lower', vmin=v_min, vmax=v_max, cmap='inferno')\n",
    "    plt.title(label[i])\n",
    "plt.gcf().set_size_inches(15, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
